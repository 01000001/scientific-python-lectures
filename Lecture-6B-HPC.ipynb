{
 "metadata": {
  "name": "Lecture-6B-HPC"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lecture 6B - Tools for high-performance computing applications\n",
      "\n",
      "J.R. Johansson (robert@riken.jp) http://dml.riken.jp/~rob/\n",
      "\n",
      "The latest version of this [IPython notebook](http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html) lecture is available at [http://github.com/jrjohansson/scientific-python-lectures](http://github.com/jrjohansson/scientific-python-lectures).\n",
      "\n",
      "The other notebooks in this lecture series are indexed at [http://jrjohansson.github.com](http://jrjohansson.github.com)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## multiprocessing\n",
      "\n",
      "Python has a built-in process-based library for concurrent computing, called `multiprocessing`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multiprocessing\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def task(args):\n",
      "    print \"PID =\", os.getpid(), \", args =\", args\n",
      "    \n",
      "    return os.getpid(), args"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "task(\"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PID = 4214 , args = test\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "(4214, 'test')"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pool = multiprocessing.Pool(processes=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = pool.map(task, [1,2,3,4,5,6,7,8])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[(5774, 1),\n",
        " (5774, 2),\n",
        " (5774, 3),\n",
        " (5777, 4),\n",
        " (5774, 5),\n",
        " (5775, 6),\n",
        " (5774, 7),\n",
        " (5774, 8)]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The multiprocessing package is very useful for highly parallel tasks that do not need to communicate with each other, other than when sending the initial data to the pool of processes and when and collecting the results. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MPI \n",
      "\n",
      "When more communication between processes is required, a more sophisticated solutions such as MPI and OpenMP is better. MPI is process based parallel processing library/protocol, and can be used in Python programs through the `mpi4py` package:\n",
      "\n",
      "http://mpi4py.scipy.org/\n",
      "\n",
      "To use the `mpi4py` package we include `MPI` from `mpi4py`:\n",
      "\n",
      "    from mpi4py import MPI\n",
      "\n",
      "A MPI python program must be started using the `mpirun -n N` command, where `N` is the number of processes that should be included in the process group."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpitest.py\n",
      "\n",
      "from mpi4py import MPI\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "if rank == 0:\n",
      "   data = [1.0, 2.0, 3.0, 4.0]\n",
      "   comm.send(data, dest=1, tag=11)\n",
      "elif rank == 1:\n",
      "   data = comm.recv(source=0, tag=11)\n",
      "    \n",
      "print \"rank =\", rank, \", data =\", data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mpitest.py\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 2 python mpitest.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rank = 0 , data = [1.0, 2.0, 3.0, 4.0]\r\n",
        "rank = 1 , data = [1.0, 2.0, 3.0, 4.0]\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 2\n",
      "\n",
      "Send a numpy array from one process to another:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi-numpy-array.py\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "if rank == 0:\n",
      "   data = numpy.random.rand(10)\n",
      "   comm.Send(data, dest=1, tag=13)\n",
      "elif rank == 1:\n",
      "   data = numpy.empty(10, dtype=numpy.float64)\n",
      "   comm.Recv(data, source=0, tag=13)\n",
      "    \n",
      "print \"rank =\", rank, \", data =\", data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mpi-numpy-array.py\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 2 python mpi-numpy-array.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rank = 0 , data = [ 0.80180329  0.99779437  0.56051888  0.65153106  0.46401712  0.25048027\r\n",
        "  0.94768137  0.75530992  0.53743554  0.22052583]\r\n",
        "rank = 1 , data = [ 0.80180329  0.99779437  0.56051888  0.65153106  0.46401712  0.25048027\r\n",
        "  0.94768137  0.75530992  0.53743554  0.22052583]\r\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example 3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi-matrix-vector.py\n",
      "# example from: http://mpi4py.scipy.org/docs/usrman/tutorial.html\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy\n",
      "\n",
      "def matvec(comm, A, x):\n",
      "    m = A.shape[0] # local rows\n",
      "    p = comm.Get_size()\n",
      "    xg = numpy.zeros(m*p, dtype='d')\n",
      "    comm.Allgather([x,  MPI.DOUBLE], [xg, MPI.DOUBLE])\n",
      "    y = numpy.dot(A, xg)\n",
      "    return y\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "A = numpy.random.rand(5,5)\n",
      "x = numpy.random.rand(5)\n",
      "y_mpi = matvec(comm, A, x)\n",
      "    \n",
      "if rank == 0:\n",
      "\n",
      "    y = numpy.dot(A, x)\n",
      "\n",
      "    print \"y =\", y\n",
      "\n",
      "    print \"y_mpi =\", y_mpi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mpi-matrix-vector.py"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 1 python mpi-matrix-vector.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A shape = (5, 5)\r\n",
        "xg shape = (5,)\r\n",
        "y = [ 1.13094626  0.51523797  1.15205033  1.05638469  0.90141265]\r\n",
        "y_mpi = [ 1.13094626  0.51523797  1.15205033  1.05638469  0.90141265]\r\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exmaple 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mpi-psum.py\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy as np\n",
      "\n",
      "def psum(a):\n",
      "    r = MPI.COMM_WORLD.Get_rank()\n",
      "    size = MPI.COMM_WORLD.Get_size()\n",
      "    m = len(a) / size\n",
      "    \n",
      "    locsum = np.sum(a[r*m:(r+1)*m])\n",
      "    rcvBuf = np.array(0.0,'d')\n",
      "    MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE], [rcvBuf, MPI.DOUBLE], op=MPI.SUM)\n",
      "    return rcvBuf\n",
      "\n",
      "a = np.random.rand(100)\n",
      "s = psum(a)\n",
      "\n",
      "if MPI.COMM_WORLD.Get_rank() == 0:\n",
      "    print \"sum =\", s, \", numpy sum =\", s.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mpi-psum.py"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mpirun -n 4 python mpi-psum.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sum = 44.525904767 , numpy sum = 44.525904767\r\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Further reading\n",
      "\n",
      "* http://mpi4py.scipy.org\n",
      "\n",
      "* http://mpi4py.scipy.org/docs/usrman/tutorial.html\n",
      "\n",
      "* https://computing.llnl.gov/tutorials/mpi/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## OpenMP\n",
      "\n",
      "What about OpenMP? OpenMP is a standard and widely spread thread-based parallel API that unfortunaltely is **not** useful in Python. The reason is that the CPython implementation use a global interpreter lock, making it impossible to simultaneously run several Python threads. Threads are therefore not useful for parallel computing in Python, unless it is only used to wrap compiled code that do the OpenMP parallelization (Numpy does something like that). \n",
      "\n",
      "This is clearly a limitation in the Python interpreter, and as a consequence all parallelization in Python must use processes (not threads)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## OpenCL\n",
      "\n",
      "OpenCL is an API for heterogenous computing, for example using GPUs for numerical computations. There is a python package called `pyopencl` that allows OpenCL code to be compiled, loaded and executed on the compute units completely from within Python. This is a nice way to work with OpenCL, because the time-consuming computations should be done on the compute units in compiled code, and in this Python only server as a control language. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file opencl-dense-mv.py\n",
      "\n",
      "import pyopencl as cl\n",
      "\n",
      "# platform\n",
      "platform_list = cl.get_platforms()\n",
      "platform = platform_list[0]\n",
      "\n",
      "# device\n",
      "device_list = platform.get_devices()\n",
      "device = device_list[0]\n",
      "\n",
      "if False:\n",
      "    print(\"Platform name:\" + platform.name)\n",
      "    print(\"Platform version:\" + platform.version)\n",
      "    print(\"Device name:\" + device.name)\n",
      "    print(\"Device type:\" + cl.device_type.to_string(device.type))\n",
      "    print(\"Device memory: \" + str(device.global_mem_size//1024//1024) + ' MB')\n",
      "    print(\"Device max clock speed:\" + str(device.max_clock_frequency) + ' MHz')\n",
      "    print(\"Device compute units:\" + str(device.max_compute_units))\n",
      "\n",
      "# context\n",
      "ctx = cl.Context([device]) # or we can use cl.create_some_context()\n",
      "\n",
      "# command queue\n",
      "queue = cl.CommandQueue(ctx)\n",
      "\n",
      "# kernel\n",
      "KERNEL_CODE = \"\"\"\n",
      "//\n",
      "// Matrix-vector multiplication: r = m * v\n",
      "//\n",
      "#define N %(mat_size)d\n",
      "__kernel\n",
      "void dmv_cl(__global float *m, __global float *v, __global float *r)\n",
      "{\n",
      "    int i, gid = get_global_id(0);\n",
      "    \n",
      "    r[gid] = 0;\n",
      "    for (i = 0; i < N; i++)\n",
      "    {\n",
      "        r[gid] += m[gid * N + i] * v[i];\n",
      "    }\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "kernel_params = {\"mat_size\": n}\n",
      "program = cl.Program(ctx, KERNEL_CODE % kernel_params).build()\n",
      "\n",
      "# data\n",
      "A = numpy.random.rand(5000, 5000)\n",
      "x = numpy.random.rand(5000, 1)\n",
      "\n",
      "# host buffers\n",
      "h_y = numpy.empty(shape(x)).astype(numpy.float32)\n",
      "h_A = real(A).astype(numpy.float32)\n",
      "h_x = real(x).astype(numpy.float32)\n",
      "\n",
      "mf = cl.mem_flags\n",
      "d_A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_A)\n",
      "d_x_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=h_x)\n",
      "d_y_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=h_y.nbytes)\n",
      "\n",
      "t0 = time.time()\n",
      "#block_size = 16\n",
      "#event = program.dmv_cl(queue, h_m.shape, (block_size, block_size), d_m_buf, d_v_buf, d_r_buf)\n",
      "event = program.dmv_cl(queue, h_r.shape, None, d_m_buf, d_v_buf, d_r_buf)\n",
      "event.wait()\n",
      "cl.enqueue_copy(queue, h_r, d_r_buf)\n",
      "t1 = time.time()\n",
      "\n",
      "print \"opencl dense: elapsed time =\", (t1-t0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing opencl-dense-mv.py"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python opencl-dense-mv.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Further reading\n",
      "\n",
      "* http://mathema.tician.de/software/pyopencl"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}